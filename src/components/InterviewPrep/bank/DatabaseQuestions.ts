import { Question } from "../InterviewSubjects";

// Collection of Database interview questions
export const databaseQuestions: Question[] = [
  {
    id: "db-1",
    question:
      "Compare SQL and NoSQL databases. When would you choose one over the other?",
    category: "technical",
    difficulty: "medium",
    type: "technical",
    sampleAnswer:
      "SQL databases are relational, table-based systems with fixed schemas that ensure data integrity through ACID transactions. They excel at complex queries and relationships between data. Examples include PostgreSQL, MySQL, and SQL Server. NoSQL databases are non-relational with flexible schemas, designed for specific data models like document, key-value, column-family, or graph. They prioritize scalability and performance over consistency. I'd choose SQL for applications requiring complex queries, transactions, and data integrity (like financial systems or inventory management). I'd opt for NoSQL when dealing with large volumes of unstructured data, requiring horizontal scaling, rapid development with changing data requirements, or specific data models like document storage (MongoDB), real-time big data (Cassandra), or graph relationships (Neo4j).",
    tips: [
      "Compare scaling approaches: vertical vs horizontal",
      "Discuss CAP theorem trade-offs",
      "Provide specific database examples",
      "Mention polyglot persistence approach",
    ],
    tags: ["database", "sql", "nosql", "systems"],
    estimatedTime: 4,
    industry: ["tech"],
    practiceCount: 0,
    successRate: 0,
  },
  {
    id: "db-2",
    question: "Explain database normalization and its normal forms.",
    category: "technical",
    difficulty: "medium",
    type: "technical",
    sampleAnswer:
      "Database normalization is the process of structuring a relational database to reduce data redundancy and improve data integrity by dividing large tables into smaller, related ones and defining relationships between them. The main normal forms are: First Normal Form (1NF) requires atomic (indivisible) values and no repeating groups. Each cell contains a single value, and each record is unique. Second Normal Form (2NF) builds on 1NF by removing partial dependencies—non-key attributes must depend on the entire primary key, not just part of it. This typically involves creating separate tables for sets of values that apply to multiple records. Third Normal Form (3NF) removes transitive dependencies—non-key attributes can't depend on other non-key attributes. Attributes should depend directly on the primary key, not through another attribute. Boyce-Codd Normal Form (BCNF) is a stricter version of 3NF, addressing anomalies not handled by 3NF in rare cases with multiple candidate keys. Fourth (4NF) and Fifth Normal Forms (5NF) deal with multi-valued dependencies and join dependencies respectively but are rarely used in practice. While normalization reduces redundancy and improves integrity, it can complicate queries and impact performance. Sometimes strategic denormalization is applied to improve read performance in analytical systems or data warehouses.",
    tips: [
      "Give examples of each normal form",
      "Discuss when denormalization might be appropriate",
      "Explain anomalies that normalization prevents",
      "Address performance considerations",
    ],
    tags: ["database", "sql", "normalization", "data-modeling"],
    estimatedTime: 5,
    industry: ["tech"],
    practiceCount: 0,
    successRate: 0,
  },
  {
    id: "db-3",
    question:
      "What are database indexes? How do they work and when should you use them?",
    category: "technical",
    difficulty: "medium",
    type: "technical",
    sampleAnswer:
      "Database indexes are data structures that improve the speed of data retrieval operations by allowing the database engine to find data without scanning the entire table. They work similarly to a book's index, providing a direct path to data locations. Most databases implement indexes using B-trees, which maintain sorted data and allow searches, insertions, and deletions in logarithmic time. When a query includes a WHERE clause on an indexed column, the database uses the index to quickly locate matching rows instead of scanning the whole table. Indexes should be created on columns frequently used in WHERE clauses, JOIN conditions, and ORDER BY statements. They're especially valuable for large tables where sequential scans become expensive. However, indexes come with trade-offs: they consume additional storage space, and they slow down write operations (INSERT, UPDATE, DELETE) because the index must be updated along with the table. Creating too many indexes can be counterproductive. Composite indexes (spanning multiple columns) are useful for queries that filter or sort by multiple columns. Covering indexes include all columns needed by a query, allowing the database to satisfy the query using only the index without accessing the table. Most databases also support specialized indexes like hash indexes (for exact equality comparisons), full-text indexes (for text searching), and spatial indexes (for geographic data).",
    tips: [
      "Explain different index types (B-tree, hash, bitmap, etc.)",
      "Discuss index selectivity and its importance",
      "Address index maintenance and fragmentation",
      "Demonstrate how to analyze index usage",
    ],
    tags: ["database", "sql", "indexes", "performance", "optimization"],
    estimatedTime: 4,
    industry: ["tech"],
    practiceCount: 0,
    successRate: 0,
  },
  {
    id: "db-4",
    question: "Explain ACID properties in database transactions.",
    category: "technical",
    difficulty: "medium",
    type: "technical",
    sampleAnswer:
      "ACID is an acronym for four properties that guarantee reliable transaction processing in databases: Atomicity ensures that transactions are treated as a single, indivisible unit—either all operations within the transaction are completed, or none are. If any part fails, the entire transaction is rolled back, preventing partial updates. Consistency ensures that transactions only transition the database from one valid state to another, maintaining all predefined rules, constraints, triggers, and cascades. Isolation ensures that concurrent transactions execute as if they were sequential, preventing transactions from seeing the intermediate states of other transactions. Database systems implement isolation levels (Read Uncommitted, Read Committed, Repeatable Read, Serializable) that trade concurrency for consistency. Durability guarantees that once a transaction is committed, it remains committed even in the event of system failure, power outage, or crashes. Committed data is written to non-volatile storage. ACID properties are crucial for applications requiring high data reliability, like financial systems, but may impose performance overhead. NoSQL databases often relax some ACID guarantees (particularly consistency and isolation) to achieve better performance and availability, following the BASE (Basically Available, Soft state, Eventually consistent) model instead.",
    tips: [
      "Compare isolation levels and their trade-offs",
      "Discuss how databases implement durability",
      "Explain anomalies like dirty reads, non-repeatable reads, and phantom reads",
      "Address distributed transaction challenges",
    ],
    tags: ["database", "transactions", "acid", "consistency"],
    estimatedTime: 4,
    industry: ["tech"],
    practiceCount: 0,
    successRate: 0,
  },
  {
    id: "db-5",
    question: "What is database sharding and how does it work?",
    category: "technical",
    difficulty: "hard",
    type: "technical",
    sampleAnswer:
      "Database sharding is a horizontal partitioning technique that splits a large database into smaller, faster, and more manageable pieces called shards across multiple servers. Each shard contains a subset of the data based on a shard key (like user ID, geographic region, or date range). Sharding architectures can be hash-based (distributing data evenly using a hash function on the shard key), range-based (partitioning by value ranges), directory-based (maintaining a lookup service that maps keys to shards), or geographic (distributing data by physical location). The main benefits include improved performance through parallel processing, increased availability (a single shard failure doesn't take down the entire database), and unlimited horizontal scaling by adding more shards. However, sharding introduces significant complexity: cross-shard queries are challenging and often inefficient; transactions spanning multiple shards are complex or impossible; resharding when capacity changes requires careful data migration; and joins across shards become difficult, often requiring application-level processing. Effective sharding requires choosing an appropriate shard key that distributes data and queries evenly to prevent hot spots. Common sharding technologies include MongoDB's native sharding, Vitess for MySQL, Citus for PostgreSQL, and custom implementations in distributed databases like Cassandra and DynamoDB.",
    tips: [
      "Compare sharding to other scaling techniques (replication, partitioning)",
      "Discuss shard key selection strategies",
      "Explain resharding challenges and approaches",
      "Address cross-shard consistency issues",
    ],
    tags: ["database", "scaling", "distributed-systems", "architecture"],
    estimatedTime: 5,
    industry: ["tech"],
    practiceCount: 0,
    successRate: 0,
  },
  {
    id: "db-6",
    question:
      "What are database transaction isolation levels and their implications?",
    category: "technical",
    difficulty: "hard",
    type: "technical",
    sampleAnswer:
      "Transaction isolation levels define how and when changes made by one transaction become visible to other concurrent transactions. The SQL standard defines four isolation levels, each with different trade-offs between consistency and performance. Read Uncommitted is the lowest isolation level, allowing transactions to see uncommitted changes from other transactions (dirty reads), non-repeatable reads, and phantom reads. This provides maximum concurrency but minimal consistency. Read Committed prevents dirty reads by ensuring a transaction only sees committed data, but still allows non-repeatable reads and phantom reads. This is the default in many databases like PostgreSQL and SQL Server. Repeatable Read prevents both dirty reads and non-repeatable reads by ensuring any row read during a transaction will remain unchanged if read again, but still allows phantom reads (new rows appearing in repeated range queries). This is the default in MySQL/InnoDB. Serializable is the highest isolation level, preventing all concurrency anomalies by effectively executing transactions in series. It prevents dirty reads, non-repeatable reads, and phantom reads but has the highest impact on performance due to reduced concurrency. Databases implement isolation using techniques like locking (shared, exclusive, predicate locks) or multi-version concurrency control (MVCC). Higher isolation levels generally mean more locks or version checking, potentially reducing throughput and increasing the chance of deadlocks. When choosing an isolation level, you must balance consistency requirements against performance needs—higher levels ensure data integrity but may reduce concurrency and increase contention.",
    tips: [
      "Explain concurrency anomalies (dirty reads, non-repeatable reads, phantom reads)",
      "Compare how different databases implement isolation levels",
      "Discuss the performance impact of each isolation level",
      "Provide scenarios where each level is appropriate",
    ],
    tags: ["database", "transactions", "concurrency", "isolation-levels"],
    estimatedTime: 5,
    industry: ["tech"],
    practiceCount: 0,
    successRate: 0,
  },
  {
    id: "db-7",
    question:
      "Explain the CAP theorem and its implications for distributed databases.",
    category: "technical",
    difficulty: "hard",
    type: "technical",
    sampleAnswer:
      "The CAP theorem, formulated by Eric Brewer, states that a distributed database system can simultaneously guarantee at most two of these three properties: Consistency (all nodes see the same data at the same time), Availability (every request receives a response, without guarantee that it contains the most recent data), and Partition tolerance (the system continues to operate despite network partitions). Since network partitions are unavoidable in distributed systems, we essentially must choose between consistency and availability when partitions occur. CP systems prioritize consistency over availability—they return an error or timeout if consistency cannot be guaranteed during a partition. Examples include traditional RDBMS with distributed transactions, MongoDB in strong consistency mode, and HBase. AP systems prioritize availability over consistency—they continue operating during partitions but may serve stale data. Examples include Cassandra, Amazon Dynamo, and CouchDB. Some systems offer tunable consistency, allowing different trade-offs for different operations. Modern distributed databases often implement more nuanced approaches beyond strict CAP categorization. For instance, PACELC extends CAP by considering latency vs. consistency trade-offs when the network is healthy. Byzantine fault tolerance addresses malicious failures beyond simple partitions. Practically, this means application architects must understand consistency requirements and make appropriate database choices—using strongly consistent systems for financial data but perhaps eventually consistent systems for user analytics or caching.",
    tips: [
      "Explain why partition tolerance is generally considered unavoidable",
      "Provide real-world examples of CP and AP systems",
      "Discuss eventual consistency and conflict resolution strategies",
      "Address how modern distributed systems navigate CAP trade-offs",
    ],
    tags: ["database", "distributed-systems", "cap-theorem", "consistency"],
    estimatedTime: 4,
    industry: ["tech"],
    practiceCount: 0,
    successRate: 0,
  },
  {
    id: "db-8",
    question: "What are database deadlocks and how can they be prevented?",
    category: "technical",
    difficulty: "medium",
    type: "technical",
    sampleAnswer:
      "A database deadlock occurs when two or more transactions are waiting for each other to release locks, creating a circular dependency where none can proceed. For example, Transaction A holds a lock on resource X and requests a lock on resource Y, while simultaneously Transaction B holds a lock on Y and requests a lock on X—both transactions block indefinitely. Most database systems automatically detect deadlocks using timeout mechanisms or dependency graph analysis, then resolve them by choosing a 'victim' transaction to abort and rollback. Several strategies can prevent or minimize deadlocks: Access resources in a consistent order—if all transactions acquire locks in the same sequence, circular wait conditions cannot occur. Keep transactions short and simple to reduce the duration of locks and minimize overlap between transactions. Use appropriate isolation levels—lower isolation levels generally acquire fewer locks. Implement lock timeouts to automatically abort transactions if they wait too long for a lock. Use optimistic concurrency control where appropriate, which avoids locks during the read phase and only checks for conflicts during the commit phase. Consider using lock escalation carefully, as converting many fine-grained locks to fewer coarse-grained locks can reduce deadlock probability but might increase contention. For applications with known deadlock issues, implementing application-level retry logic can help recover gracefully when the database's deadlock resolution mechanism aborts a transaction.",
    tips: [
      "Explain the four conditions necessary for deadlocks to occur",
      "Discuss how different databases handle deadlock detection and resolution",
      "Compare deadlock prevention strategies and their trade-offs",
      "Demonstrate how to analyze deadlocks using database tools",
    ],
    tags: ["database", "concurrency", "deadlocks", "transactions"],
    estimatedTime: 4,
    industry: ["tech"],
    practiceCount: 0,
    successRate: 0,
  },
  {
    id: "db-9",
    question: "Explain database partitioning strategies and their use cases.",
    category: "technical",
    difficulty: "medium",
    type: "technical",
    sampleAnswer:
      "Database partitioning divides large tables into smaller, more manageable pieces called partitions, while maintaining a single logical view of the data. Horizontal partitioning (sharding) splits a table by rows across multiple locations based on a partition key. Common strategies include: Range partitioning divides data based on value ranges (e.g., dates, ID ranges), ideal for time-series data or historical archiving; Hash partitioning applies a hash function to the partition key for even distribution, good for avoiding hotspots when there's no natural range; List partitioning assigns rows based on discrete values in a column, useful for categorical data like regions or status; Composite partitioning combines multiple strategies, like hash+range. Vertical partitioning splits a table by columns, storing frequently accessed columns separately from rarely accessed ones. This improves cache utilization and I/O efficiency. Partitioning offers several benefits: improved query performance through partition pruning (skipping irrelevant partitions), parallel operations across partitions, easier maintenance (adding/removing partitions without downtime), and efficient archiving of old data. However, it adds complexity to database design and can complicate operations that span multiple partitions. Most modern databases support various partitioning schemes: PostgreSQL and Oracle offer declarative table partitioning; MySQL supports explicit partitioning functions; and distributed databases like Cassandra and Citus implement partitioning as a fundamental feature. When designing a partitioning strategy, consider query patterns, data distribution, growth rates, and maintenance operations to choose the most appropriate approach.",
    tips: [
      "Contrast partitioning with sharding and replication",
      "Explain partition pruning and its performance benefits",
      "Discuss common partition maintenance operations",
      "Address challenges with joins across partitions",
    ],
    tags: ["database", "partitioning", "performance", "data-architecture"],
    estimatedTime: 4,
    industry: ["tech"],
    practiceCount: 0,
    successRate: 0,
  },
  {
    id: "db-10",
    question: "What are database constraints and why are they important?",
    category: "technical",
    difficulty: "easy",
    type: "technical",
    sampleAnswer:
      "Database constraints are rules enforced by the database management system to maintain data integrity and accuracy. The primary types include: Primary Key constraints ensure each row in a table is uniquely identified by one or more columns that cannot contain NULL values. Unique constraints ensure all values in a column or group of columns are distinct, though they may contain NULLs (unlike primary keys). Foreign Key constraints establish relationships between tables by requiring values in one table to match values in another table's primary key or unique constraint. Check constraints enforce specific conditions on column values using predicates (e.g., price > 0). Not NULL constraints prevent columns from accepting NULL values. Default constraints provide automatic values when no explicit value is specified during insertion. Domain constraints limit the range of allowed values for a column. Constraints are crucial for several reasons: They ensure data integrity by preventing invalid data entry at the database level rather than relying solely on application validation. They provide a self-documenting aspect to the database schema by explicitly defining rules and relationships. They improve query optimizer performance by providing useful information about data characteristics. They centralize business rules in the database, ensuring consistent enforcement regardless of which application accesses the data. While constraints can be implemented in application code, database-level constraints are more reliable because they cannot be bypassed and remain in effect regardless of how the data is accessed or modified.",
    tips: [
      "Compare implementing constraints in databases versus application code",
      "Discuss performance implications of different constraint types",
      "Explain the difference between deferrable and non-deferrable constraints",
      "Provide examples of common constraint usage patterns",
    ],
    tags: ["database", "constraints", "data-integrity", "database-design"],
    estimatedTime: 3,
    industry: ["tech"],
    practiceCount: 0,
    successRate: 0,
  },
  {
    id: "db-11",
    question:
      "What is database replication and what are the common replication topologies?",
    category: "technical",
    difficulty: "medium",
    type: "technical",
    sampleAnswer:
      "Database replication creates and maintains copies of a database across multiple servers, primarily for improved availability, fault tolerance, and read performance. Replication can be synchronous, where primary servers wait for replicas to confirm changes before committing (ensuring consistency but increasing latency), or asynchronous, where primary servers commit immediately without waiting (improving performance but potentially allowing data loss during failures). Common replication topologies include: Single-Primary (Master-Slave) replication, where one primary server handles all write operations while multiple replicas serve read queries. This provides read scalability and basic failover capabilities. Multi-Primary (Multi-Master) replication, where multiple servers accept write operations, offering higher write availability but introducing complex conflict resolution requirements. Cascading replication, where replicas themselves have replicas, reducing load on the primary server but increasing replication lag. Circular replication, where servers form a ring of replication connections, providing redundancy against single-point failures. Star replication, where a central server replicates to multiple independent branches, useful for geographic distribution. Each approach has trade-offs regarding consistency, availability, latency, and complexity. Modern implementations include statement-based replication (copying SQL commands), row-based replication (copying changed rows), or logical replication (using a publisher-subscriber model with change data capture). Replication challenges include handling schema changes across replicas, managing replication lag, conflict resolution in multi-primary setups, and failover procedures. Most major databases support replication natively: MySQL with binary logs, PostgreSQL with streaming replication, SQL Server with AlwaysOn, and MongoDB with replica sets.",
    tips: [
      "Compare synchronous vs. asynchronous replication trade-offs",
      "Discuss replication lag monitoring and mitigation strategies",
      "Explain failover mechanisms and automated failover solutions",
      "Address read consistency issues with asynchronous replicas",
    ],
    tags: [
      "database",
      "replication",
      "high-availability",
      "distributed-systems",
    ],
    estimatedTime: 4,
    industry: ["tech"],
    practiceCount: 0,
    successRate: 0,
  },
  {
    id: "db-12",
    question: "Explain database joins and different types of joins in SQL.",
    category: "technical",
    difficulty: "medium",
    type: "technical",
    sampleAnswer:
      "SQL joins combine rows from two or more tables based on related columns, allowing queries to retrieve data from multiple tables in a single operation. The main types are: INNER JOIN returns only rows that have matching values in both tables. For example, joining customers and orders on customer_id returns only customers who have placed orders. LEFT JOIN (or LEFT OUTER JOIN) returns all rows from the left table and matching rows from the right table. If no match exists, NULL values are returned for right table columns. This shows all customers and their orders, including customers with no orders. RIGHT JOIN (or RIGHT OUTER JOIN) returns all rows from the right table and matching rows from the left table, with NULLs for non-matches from the left table. This shows all orders with their customer details, even for orders without valid customers. FULL JOIN (or FULL OUTER JOIN) returns all rows when there's a match in either table, with NULLs for non-matching sides. This shows all customers and all orders, regardless of matches. CROSS JOIN creates a Cartesian product, combining each row from the first table with every row from the second table. This results in m×n rows and has specialized uses like generating combination tables. SELF JOIN is not a distinct join type but a regular join where a table is joined with itself, useful for hierarchical data like employee-manager relationships. Joins can be implemented using different algorithms: nested loop joins (for small tables or indexed columns), hash joins (for larger tables without indexes), and merge joins (for sorted data). The choice of join type significantly impacts query results and performance, especially with large tables. Modern SQL also supports additional join variations like NATURAL JOIN, which automatically joins tables on columns with the same name.",
    tips: [
      "Use Venn diagrams to visualize different join types",
      "Explain join performance considerations and optimization",
      "Discuss the importance of indexes for join operations",
      "Demonstrate common join patterns with examples",
    ],
    tags: ["database", "sql", "joins", "query-optimization"],
    estimatedTime: 4,
    industry: ["tech"],
    practiceCount: 0,
    successRate: 0,
  },
  {
    id: "db-13",
    question:
      "What is a database index? How do you decide which columns to index?",
    category: "technical",
    difficulty: "medium",
    type: "technical",
    sampleAnswer:
      "A database index is a data structure that improves the speed of data retrieval operations by providing a fast path to rows in a database table, similar to a book's index. When deciding which columns to index, several factors should be considered: Frequency of column usage in WHERE clauses, JOIN conditions, and ORDER BY/GROUP BY statements—columns frequently used for filtering or joining are prime index candidates. Query patterns—analyze which queries are most common and performance-critical for your application. Cardinality (uniqueness)—columns with high cardinality (many distinct values) make better index candidates than low-cardinality columns. For example, a 'gender' column with only two values would provide minimal filtering benefit. Selectivity—how effective an index is at narrowing down results. An index on a column where queries typically select a small percentage of rows (high selectivity) is more beneficial. Update frequency—indexes slow down write operations as they must be maintained, so consider the read/write ratio of the table. Column size—smaller columns make more efficient indexes. Composite indexes should be considered when queries filter by multiple columns together, with the most selective columns listed first. Covering indexes (including all columns needed by a query) can dramatically improve performance by avoiding table access entirely. However, over-indexing should be avoided as it consumes storage space and slows down data modification operations. Regular index usage analysis using database monitoring tools helps identify unused indexes that can be removed and missing indexes that should be created. Specialized index types like partial indexes, expression indexes, or full-text indexes may be appropriate for specific use cases.",
    tips: [
      "Discuss how to use database monitoring tools to identify index candidates",
      "Explain the trade-off between read and write performance with indexes",
      "Compare different index types (B-tree, hash, GiST, etc.) and their use cases",
      "Demonstrate how to analyze query execution plans to assess index usage",
    ],
    tags: ["database", "sql", "indexes", "performance-tuning"],
    estimatedTime: 4,
    industry: ["tech"],
    practiceCount: 0,
    successRate: 0,
  },
  {
    id: "db-14",
    question:
      "Write a SQL query to find the second highest salary in an employees table.",
    category: "technical",
    difficulty: "medium",
    type: "technical",
    sampleAnswer:
      "There are several ways to find the second highest salary in an employees table. One of the most efficient approaches uses the LIMIT and OFFSET clauses: SELECT DISTINCT salary FROM employees WHERE salary IS NOT NULL ORDER BY salary DESC LIMIT 1 OFFSET 1; This orders salaries in descending order and skips the first result (the highest salary) to get the second highest. Another approach uses a subquery to find the maximum salary, then finds the maximum among the remaining salaries: SELECT MAX(salary) FROM employees WHERE salary < (SELECT MAX(salary) FROM employees); This works well across most database systems. For handling ties (multiple employees with the same salary), we can use dense_rank(): SELECT salary FROM (SELECT salary, DENSE_RANK() OVER (ORDER BY salary DESC) as rank FROM employees) ranked_salaries WHERE rank = 2; This assigns the same rank to identical salaries and returns all salaries with rank 2. If we want exactly one result even with ties, we can use: SELECT salary FROM employees e1 WHERE 2 = (SELECT COUNT(DISTINCT salary) FROM employees e2 WHERE e2.salary >= e1.salary); This counts how many distinct salaries are greater than or equal to the current salary, returning the one where exactly two distinct values meet this condition. Each approach has trade-offs: the LIMIT/OFFSET method is simple but doesn't handle nulls or ties well; the subquery method works across databases but performs two full table scans; and the window function approach handles ties properly but isn't supported in all database versions.",
    tips: [
      "Explain how each solution handles edge cases like NULL salaries or ties",
      "Discuss how the query would change for the Nth highest salary",
      "Compare the performance characteristics of different solutions",
      "Demonstrate how to test the query with sample data",
    ],
    tags: ["database", "sql", "query", "interview-question"],
    estimatedTime: 3,
    industry: ["tech"],
    practiceCount: 0,
    successRate: 0,
  },
  {
    id: "db-15",
    question: "Write a SQL query to find duplicate values in a table.",
    category: "technical",
    difficulty: "easy",
    type: "technical",
    sampleAnswer:
      "To find duplicate values in a table, we typically use GROUP BY with the HAVING clause to count occurrences. For example, to find duplicate email addresses in a users table: SELECT email, COUNT(email) AS count FROM users GROUP BY email HAVING COUNT(email) > 1; This groups all rows by email address, then filters to show only those groups with more than one occurrence. To see all duplicate records with their complete information, we can use a self-join or a subquery: WITH duplicates AS (SELECT email FROM users GROUP BY email HAVING COUNT(*) > 1) SELECT u.* FROM users u JOIN duplicates d ON u.email = d.email ORDER BY u.email; This approach first identifies the duplicate email values, then joins back to the original table to retrieve all columns for those records. To find duplicates across multiple columns (e.g., first_name and last_name): SELECT first_name, last_name, COUNT(*) AS count FROM users GROUP BY first_name, last_name HAVING COUNT(*) > 1; For large tables, performance can be improved by creating an index on the columns being checked for duplicates. If we want to identify specific instances of duplicates uniquely, we can include row identifiers: SELECT u.id, u.email FROM users u INNER JOIN (SELECT email FROM users GROUP BY email HAVING COUNT(*) > 1) dup ON u.email = dup.email ORDER BY u.email; These queries help in data cleaning, enforcing uniqueness constraints, or identifying potential data quality issues in database tables.",
    tips: [
      "Explain variations for finding duplicates in specific columns versus entire rows",
      "Discuss approaches for handling duplicates (deletion, updating, etc.)",
      "Demonstrate how to use window functions for identifying duplicates",
      "Address performance considerations for large tables",
    ],
    tags: ["database", "sql", "data-quality", "query"],
    estimatedTime: 3,
    industry: ["tech"],
    practiceCount: 0,
    successRate: 0,
  },
  {
    id: "db-16",
    question:
      "Write a SQL query to find employees who earn more than their managers.",
    category: "technical",
    difficulty: "medium",
    type: "technical",
    sampleAnswer:
      "To find employees who earn more than their managers, we need to join the employees table with itself to compare employee and manager salaries. Assuming an employees table with columns id, name, salary, and manager_id (referencing another employee who is the manager), the query would be: SELECT e.name AS Employee, m.name AS Manager, e.salary AS EmployeeSalary, m.salary AS ManagerSalary FROM employees e JOIN employees m ON e.manager_id = m.id WHERE e.salary > m.salary; This self-join connects each employee to their manager by joining on the employee's manager_id and the manager's id. The WHERE clause filters to show only cases where the employee's salary exceeds their manager's salary. If some employees don't have managers (like the CEO), we could use a LEFT JOIN instead and add a condition to filter out NULL manager salaries: SELECT e.name AS Employee, m.name AS Manager, e.salary AS EmployeeSalary, m.salary AS ManagerSalary FROM employees e LEFT JOIN employees m ON e.manager_id = m.id WHERE e.salary > COALESCE(m.salary, 0); For performance optimization with large tables, indexes should be created on the id and manager_id columns. This query can be extended to include additional information like departments or the percentage by which an employee's salary exceeds their manager's: SELECT e.name AS Employee, m.name AS Manager, e.salary AS EmployeeSalary, m.salary AS ManagerSalary, ROUND((e.salary - m.salary) * 100.0 / m.salary, 2) AS PercentageHigher FROM employees e JOIN employees m ON e.manager_id = m.id WHERE e.salary > m.salary;",
    tips: [
      "Explain how the self-join works to connect employees with managers",
      "Discuss how to handle null manager_id values (top-level employees)",
      "Suggest additional information that might be useful in the result",
      "Explain indexing considerations for optimizing this query",
    ],
    tags: ["database", "sql", "self-join", "query"],
    estimatedTime: 3,
    industry: ["tech"],
    practiceCount: 0,
    successRate: 0,
  },
  {
    id: "db-17",
    question:
      "Write a SQL query to calculate running totals or cumulative sums.",
    category: "technical",
    difficulty: "medium",
    type: "technical",
    sampleAnswer:
      "To calculate running totals or cumulative sums in SQL, we can use window functions, which are available in most modern database systems. For example, to calculate a running total of sales amounts by date: SELECT sale_date, amount, SUM(amount) OVER (ORDER BY sale_date) AS running_total FROM sales; This returns each sale with a running total of all sales up to and including that date. For running totals within groups (e.g., by department), we add a PARTITION BY clause: SELECT sale_date, department, amount, SUM(amount) OVER (PARTITION BY department ORDER BY sale_date) AS dept_running_total FROM sales; This resets the running total for each department. For databases that don't support window functions (like older MySQL versions), we can use a self-join with a correlated subquery: SELECT s1.sale_date, s1.amount, (SELECT SUM(s2.amount) FROM sales s2 WHERE s2.sale_date <= s1.sale_date) AS running_total FROM sales s1 ORDER BY s1.sale_date; For running totals with more complex logic, such as a fiscal year that doesn't align with the calendar year, we can use computed columns within the window function: SELECT sale_date, amount, SUM(amount) OVER (ORDER BY CASE WHEN MONTH(sale_date) >= 7 THEN CONCAT(YEAR(sale_date), '-', MONTH(sale_date)) ELSE CONCAT(YEAR(sale_date)-1, '-', MONTH(sale_date)) END) AS fiscal_running_total FROM sales; Window functions generally offer better performance than self-joins for running totals, especially with large datasets, and they allow for more complex aggregations like running averages, minimums, maximums, or custom ranking.",
    tips: [
      "Compare window function approach with self-joins for different databases",
      "Explain how to handle date ranges with gaps in the running total",
      "Demonstrate calculating running totals for different time periods (daily, monthly)",
      "Discuss performance considerations for large datasets",
    ],
    tags: ["database", "sql", "window-functions", "analytics"],
    estimatedTime: 4,
    industry: ["tech", "finance"],
    practiceCount: 0,
    successRate: 0,
  },
  {
    id: "db-18",
    question: "Write a SQL query to pivot rows into columns (transpose data).",
    category: "technical",
    difficulty: "hard",
    type: "technical",
    sampleAnswer:
      "Pivoting rows into columns (transposing data) can be accomplished in SQL using conditional aggregation or PIVOT functions, depending on the database system. Let's consider a sales table with columns product, quarter, and amount, where we want to transform quarterly sales into columns. Using conditional aggregation, which works across most databases: SELECT product, SUM(CASE WHEN quarter = 'Q1' THEN amount ELSE 0 END) AS Q1_sales, SUM(CASE WHEN quarter = 'Q2' THEN amount ELSE 0 END) AS Q2_sales, SUM(CASE WHEN quarter = 'Q3' THEN amount ELSE 0 END) AS Q3_sales, SUM(CASE WHEN quarter = 'Q4' THEN amount ELSE 0 END) AS Q4_sales FROM sales GROUP BY product; For databases like SQL Server that support the PIVOT operator: SELECT product, [Q1] AS Q1_sales, [Q2] AS Q2_sales, [Q3] AS Q3_sales, [Q4] AS Q4_sales FROM ( SELECT product, quarter, amount FROM sales ) src PIVOT ( SUM(amount) FOR quarter IN ([Q1], [Q2], [Q3], [Q4]) ) pvt; When the column values are not known in advance, we need dynamic SQL. In PostgreSQL, we might use crosstab from the tablefunc extension: SELECT * FROM crosstab( 'SELECT product, quarter, amount FROM sales ORDER BY 1,2', 'SELECT DISTINCT quarter FROM sales ORDER BY 1' ) AS ct (product text, Q1 numeric, Q2 numeric, Q3 numeric, Q4 numeric); In MySQL, we could use prepared statements to build dynamic pivots. The choice between static and dynamic pivoting depends on whether the pivot columns are known in advance. For large datasets, pivoting can be resource-intensive, so consider materialized views or pre-aggregation tables for frequently accessed pivoted data.",
    tips: [
      "Compare pivoting approaches across different database systems",
      "Explain dynamic pivoting for unknown column values",
      "Discuss performance implications for large datasets",
      "Demonstrate handling null values in pivoted results",
    ],
    tags: ["database", "sql", "pivot", "data-transformation"],
    estimatedTime: 5,
    industry: ["tech", "data-analytics"],
    practiceCount: 0,
    successRate: 0,
  },
  {
    id: "db-19",
    question: "Write a SQL query to calculate the median value in a table.",
    category: "technical",
    difficulty: "hard",
    type: "technical",
    sampleAnswer:
      "Calculating the median in SQL varies by database system as there's no standard median function across all platforms. For databases with built-in median functions (like Oracle, PostgreSQL), it's straightforward: SELECT MEDIAN(salary) FROM employees; In PostgreSQL, using percentile_cont: SELECT percentile_cont(0.5) WITHIN GROUP (ORDER BY salary) FROM employees; For SQL Server, using PERCENTILE_CONT: SELECT PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY salary) OVER() FROM employees; For MySQL, which lacks native median functions, we can use a workaround with variables: SET @row_index := -1; SELECT AVG(subq.salary) as median_value FROM (SELECT @row_index:=@row_index+1 AS row_index, salary FROM employees ORDER BY salary) AS subq WHERE subq.row_index IN (FLOOR(@row_index/2), CEIL(@row_index/2)); Alternatively, we can use pure SQL with COUNT and subqueries: SELECT AVG(t1.salary) as median FROM ( SELECT e1.salary FROM employees e1, employees e2 GROUP BY e1.salary HAVING SUM(CASE WHEN e2.salary <= e1.salary THEN 1 ELSE 0 END) >= COUNT(*)/2 AND SUM(CASE WHEN e2.salary >= e1.salary THEN 1 ELSE 0 END) >= COUNT(*)/2 ) t1; This works by finding values where the count of greater-or-equal values and less-or-equal values are both at least half the total count. For large tables, these approaches can be performance-intensive. Creating a sorted index on the column or using table sampling techniques can improve performance for approximate median calculations.",
    tips: [
      "Compare solutions for different database systems",
      "Explain the difference between median calculation for odd and even counts",
      "Discuss performance optimization for large datasets",
      "Show how to calculate median within groups using PARTITION BY",
    ],
    tags: ["database", "sql", "statistics", "analytics"],
    estimatedTime: 4,
    industry: ["tech", "data-science"],
    practiceCount: 0,
    successRate: 0,
  },
  {
    id: "db-20",
    question: "Write a SQL query to find the most frequent value in a column.",
    category: "technical",
    difficulty: "easy",
    type: "technical",
    sampleAnswer:
      "To find the most frequent value in a column (the mode), we can use GROUP BY with COUNT and then order by the count in descending order. For example, to find the most common product category: SELECT category, COUNT(*) AS frequency FROM products GROUP BY category ORDER BY frequency DESC LIMIT 1; This query groups the products by category, counts occurrences in each group, and returns the category with the highest count. If there are ties (multiple categories with the same highest frequency), this only returns one result. To handle ties and return all modes: WITH frequencies AS ( SELECT category, COUNT(*) AS frequency FROM products GROUP BY category ), max_frequency AS ( SELECT MAX(frequency) AS max_freq FROM frequencies ) SELECT f.category, f.frequency FROM frequencies f JOIN max_frequency m ON f.frequency = m.max_freq ORDER BY f.category; This approach uses common table expressions (CTEs) to first calculate all frequencies, then find the maximum frequency, and finally return all categories that match this maximum. For databases without CTE support, nested subqueries can be used: SELECT category, COUNT(*) AS frequency FROM products GROUP BY category HAVING COUNT(*) = ( SELECT MAX(frequency) FROM ( SELECT COUNT(*) AS frequency FROM products GROUP BY category ) AS subq ) ORDER BY category; This pattern can be extended to find the mode within groups by adding additional grouping columns, or to find the top N most frequent values by increasing the LIMIT.",
    tips: [
      "Explain how to handle ties for the most frequent value",
      "Discuss variations for finding the top N most frequent values",
      "Demonstrate finding modes within groups (e.g., most popular product per category)",
      "Address performance considerations for large tables",
    ],
    tags: ["database", "sql", "statistics", "query"],
    estimatedTime: 3,
    industry: ["tech", "data-analysis"],
    practiceCount: 0,
    successRate: 0,
  },
  {
    id: "db-21",
    question:
      "Write a SQL query to find all departments with higher than average salary.",
    category: "technical",
    difficulty: "medium",
    type: "technical",
    sampleAnswer:
      "To find departments with higher than average salary, we need to compare each department's average salary with the overall average salary across all departments. First, let's calculate the overall average: WITH overall_avg AS ( SELECT AVG(salary) AS avg_salary FROM employees ) SELECT d.name AS department_name, AVG(e.salary) AS avg_dept_salary, o.avg_salary AS overall_avg_salary FROM departments d JOIN employees e ON d.id = e.department_id CROSS JOIN overall_avg o GROUP BY d.name, o.avg_salary HAVING AVG(e.salary) > o.avg_salary ORDER BY avg_dept_salary DESC; This query uses a CTE to calculate the overall average salary once, then joins it with departments and employees tables to compare each department's average salary to the overall average. Alternatively, using a subquery: SELECT d.name AS department_name, AVG(e.salary) AS avg_dept_salary, (SELECT AVG(salary) FROM employees) AS overall_avg_salary FROM departments d JOIN employees e ON d.id = e.department_id GROUP BY d.name HAVING AVG(e.salary) > (SELECT AVG(salary) FROM employees) ORDER BY avg_dept_salary DESC; For additional insight, we can include the percentage difference from the average: SELECT d.name AS department_name, AVG(e.salary) AS avg_dept_salary, (SELECT AVG(salary) FROM employees) AS overall_avg_salary, ROUND((AVG(e.salary) - (SELECT AVG(salary) FROM employees)) * 100.0 / (SELECT AVG(salary) FROM employees), 2) AS percentage_above_avg FROM departments d JOIN employees e ON d.id = e.department_id GROUP BY d.name HAVING AVG(e.salary) > (SELECT AVG(salary) FROM employees) ORDER BY avg_dept_salary DESC; This approach works efficiently for most database sizes. For very large tables, materializing the average calculation in a temporary table might improve performance.",
    tips: [
      "Explain how the cross join works with the CTE approach",
      "Compare CTE approach with subquery approach for performance",
      "Discuss how to extend to find departments significantly above average (e.g., >10%)",
      "Demonstrate handling NULL values in salary or department assignments",
    ],
    tags: ["database", "sql", "aggregation", "having"],
    estimatedTime: 3,
    industry: ["tech", "hr"],
    practiceCount: 0,
    successRate: 0,
  },
  {
    id: "db-22",
    question: "Write a SQL query to find employees hired in the last N months.",
    category: "technical",
    difficulty: "easy",
    type: "technical",
    sampleAnswer:
      "To find employees hired in the last N months, we use date/time functions to filter the hire_date column. The specific functions vary by database system. For a PostgreSQL example finding employees hired in the last 3 months: SELECT employee_id, first_name, last_name, hire_date FROM employees WHERE hire_date >= CURRENT_DATE - INTERVAL '3 months' ORDER BY hire_date DESC; In MySQL: SELECT employee_id, first_name, last_name, hire_date FROM employees WHERE hire_date >= DATE_SUB(CURDATE(), INTERVAL 3 MONTH) ORDER BY hire_date DESC; In SQL Server: SELECT employee_id, first_name, last_name, hire_date FROM employees WHERE hire_date >= DATEADD(month, -3, GETDATE()) ORDER BY hire_date DESC; In Oracle: SELECT employee_id, first_name, last_name, hire_date FROM employees WHERE hire_date >= ADD_MONTHS(SYSDATE, -3) ORDER BY hire_date DESC; To parameterize the number of months, we can use a variable in database systems that support it or create a stored procedure. For more nuanced date range calculations, we might need to handle month boundaries. For example, to find employees hired in complete calendar months rather than a rolling period: SELECT employee_id, first_name, last_name, hire_date FROM employees WHERE EXTRACT(YEAR FROM hire_date) = EXTRACT(YEAR FROM CURRENT_DATE) AND EXTRACT(MONTH FROM hire_date) BETWEEN EXTRACT(MONTH FROM CURRENT_DATE) - 3 AND EXTRACT(MONTH FROM CURRENT_DATE) ORDER BY hire_date DESC; This approach needs adjustment for cases where the range spans multiple years. The appropriate method depends on the specific business requirements for date calculations.",
    tips: [
      "Compare date functions across different database systems",
      "Explain the difference between rolling periods vs. calendar months",
      "Discuss time zone considerations for global applications",
      "Show how to extend the query to include additional hiring metrics",
    ],
    tags: ["database", "sql", "date-functions", "filtering"],
    estimatedTime: 2,
    industry: ["tech", "hr"],
    practiceCount: 0,
    successRate: 0,
  },
  {
    id: "db-23",
    question:
      "Write a SQL query to rank employees by salary within each department.",
    category: "technical",
    difficulty: "medium",
    type: "technical",
    sampleAnswer:
      "Ranking employees by salary within departments can be accomplished using window functions like RANK(), DENSE_RANK(), or ROW_NUMBER(). Each serves a slightly different purpose: RANK() gives the same rank to ties and skips subsequent ranks; DENSE_RANK() gives the same rank to ties but doesn't skip ranks; ROW_NUMBER() assigns unique sequential numbers regardless of ties. Using RANK(): SELECT e.employee_id, e.first_name, e.last_name, d.name AS department, e.salary, RANK() OVER (PARTITION BY e.department_id ORDER BY e.salary DESC) AS salary_rank FROM employees e JOIN departments d ON e.department_id = d.department_id ORDER BY d.name, salary_rank; This query partitions employees by department and ranks them by descending salary within each department. For databases that don't support window functions, we can use a correlated subquery: SELECT e.employee_id, e.first_name, e.last_name, d.name AS department, e.salary, (SELECT COUNT(DISTINCT e2.salary) FROM employees e2 WHERE e2.department_id = e.department_id AND e2.salary >= e.salary) AS salary_rank FROM employees e JOIN departments d ON e.department_id = d.department_id ORDER BY d.name, salary_rank; We can extend the ranking query to show additional information, such as comparing an employee's salary to the department average: SELECT e.employee_id, e.first_name, e.last_name, d.name AS department, e.salary, RANK() OVER (PARTITION BY e.department_id ORDER BY e.salary DESC) AS salary_rank, AVG(e.salary) OVER (PARTITION BY e.department_id) AS dept_avg_salary, ROUND((e.salary - AVG(e.salary) OVER (PARTITION BY e.department_id)) * 100.0 / AVG(e.salary) OVER (PARTITION BY e.department_id), 2) AS percentage_from_avg FROM employees e JOIN departments d ON e.department_id = d.department_id ORDER BY d.name, salary_rank;",
    tips: [
      "Explain the differences between RANK, DENSE_RANK, and ROW_NUMBER",
      "Demonstrate how to handle ties in rankings",
      "Discuss alternative approaches for databases without window functions",
      "Show how to identify top N employees within each department",
    ],
    tags: ["database", "sql", "window-functions", "ranking"],
    estimatedTime: 3,
    industry: ["tech", "hr"],
    practiceCount: 0,
    successRate: 0,
  },
  {
    id: "db-24",
    question:
      "Write a SQL query to calculate moving averages over time series data.",
    category: "technical",
    difficulty: "hard",
    type: "technical",
    sampleAnswer:
      "Calculating moving averages on time series data uses window functions with a frame clause to define the moving window. For a 7-day moving average of sales data: SELECT sale_date, amount, AVG(amount) OVER (ORDER BY sale_date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS moving_avg_7day FROM daily_sales ORDER BY sale_date; This computes the average of the current row and the 6 preceding rows, creating a 7-day moving window. For databases without window function support, we can use a self-join: SELECT ds1.sale_date, ds1.amount, AVG(ds2.amount) AS moving_avg_7day FROM daily_sales ds1 JOIN daily_sales ds2 ON ds2.sale_date BETWEEN ds1.sale_date - INTERVAL '6' DAY AND ds1.sale_date GROUP BY ds1.sale_date, ds1.amount ORDER BY ds1.sale_date; For a 30-day moving average centered on the current date (15 days before and 14 days after): SELECT sale_date, amount, AVG(amount) OVER (ORDER BY sale_date ROWS BETWEEN 15 PRECEDING AND 14 FOLLOWING) AS centered_moving_avg_30day FROM daily_sales ORDER BY sale_date; Moving averages can be calculated within groups using the PARTITION BY clause: SELECT sale_date, product_category, amount, AVG(amount) OVER (PARTITION BY product_category ORDER BY sale_date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS category_moving_avg_7day FROM daily_sales ORDER BY product_category, sale_date; When dealing with irregular intervals or missing dates, using RANGE instead of ROWS can help: SELECT sale_date, amount, AVG(amount) OVER (ORDER BY sale_date RANGE BETWEEN INTERVAL '7' DAY PRECEDING AND CURRENT ROW) AS moving_avg_7day FROM daily_sales ORDER BY sale_date; This calculates the average of all values within the past 7 days, regardless of how many data points exist in that period.",
    tips: [
      "Explain ROWS vs RANGE frame specifications for different use cases",
      "Discuss handling missing dates in time series data",
      "Compare different types of moving averages (simple, weighted, exponential)",
      "Demonstrate implementing weighted moving averages in SQL",
    ],
    tags: ["database", "sql", "time-series", "window-functions", "analytics"],
    estimatedTime: 4,
    industry: ["tech", "finance", "data-science"],
    practiceCount: 0,
    successRate: 0,
  },
  {
    id: "db-25",
    question: "Write a SQL query to identify and fill gaps in sequential data.",
    category: "technical",
    difficulty: "hard",
    type: "technical",
    sampleAnswer:
      "Identifying and filling gaps in sequential data (like missing dates or IDs) is a common requirement in data analysis. First, to identify gaps in sequential integers (like order_id): SELECT t1.order_id + 1 AS gap_start, MIN(t2.order_id) - 1 AS gap_end FROM orders t1 JOIN orders t2 ON t1.order_id < t2.order_id WHERE t1.order_id + 1 < t2.order_id GROUP BY t1.order_id ORDER BY t1.order_id; This query finds consecutive IDs where the difference is greater than 1, indicating a gap. For date ranges, we can generate a complete series of dates and left join to find missing dates: WITH date_series AS ( SELECT generate_series( (SELECT MIN(sale_date) FROM sales), (SELECT MAX(sale_date) FROM sales), INTERVAL '1 day' )::date AS calendar_date ) SELECT ds.calendar_date, COALESCE(s.amount, 0) AS amount FROM date_series ds LEFT JOIN sales s ON ds.calendar_date = s.sale_date ORDER BY ds.calendar_date; This generates all dates between the minimum and maximum in the sales table, then left joins to show 0 for missing dates. The date generation function varies by database: generate_series() in PostgreSQL, recursive CTEs in SQL Server and MySQL. To fill gaps with interpolated values, we can use window functions: WITH date_series AS ( SELECT generate_series( (SELECT MIN(sale_date) FROM sales), (SELECT MAX(sale_date) FROM sales), INTERVAL '1 day' )::date AS calendar_date ), joined_data AS ( SELECT ds.calendar_date, s.amount FROM date_series ds LEFT JOIN sales s ON ds.calendar_date = s.sale_date ORDER BY ds.calendar_date ) SELECT calendar_date, COALESCE(amount, LAST_VALUE(amount IGNORE NULLS) OVER (ORDER BY calendar_date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)) AS filled_amount FROM joined_data ORDER BY calendar_date; This carries forward the last known value to fill gaps (known as last observation carried forward or LOCF).",
    tips: [
      "Explain different gap-filling strategies (LOCF, linear interpolation, average)",
      "Compare techniques for generating sequences across database systems",
      "Discuss handling first/last values where carry-forward/backward isn't possible",
      "Show how to identify patterns in gaps that might indicate data issues",
    ],
    tags: ["database", "sql", "data-cleansing", "time-series"],
    estimatedTime: 5,
    industry: ["tech", "data-science"],
    practiceCount: 0,
    successRate: 0,
  },
  {
    id: "db-26",
    question:
      "Explain the concept of database locking and its importance in transaction management.",
    category: "technical",
    difficulty: "medium",
    type: "technical",
    sampleAnswer:
      "Database locking is a mechanism that prevents multiple transactions from concurrently accessing or modifying the same data in ways that would violate data integrity. Locks restrict access to data resources during transactions, ensuring consistency in multi-user environments. The primary lock types are: Shared (read) locks allow multiple transactions to read data simultaneously but prevent any transaction from modifying that data while the lock is held. Exclusive (write) locks prevent any other transaction from reading or writing the locked data until the lock is released. Intent locks signal that a transaction plans to acquire locks at a finer granularity level, improving lock management efficiency. Update locks are initially compatible with shared locks but can be promoted to exclusive locks, reducing deadlock probability. Key locking levels include: Row-level locks affect individual rows, providing maximum concurrency but higher overhead. Page-level locks lock physical database pages containing multiple rows. Table-level locks affect entire tables, reducing overhead but limiting concurrency. Database-level locks affect the entire database. Most systems use multi-version concurrency control (MVCC) or optimistic locking to reduce locking contention. Optimistic locking assumes conflicts are rare and verifies this assumption at commit time, while pessimistic locking acquires locks before operations. Lock management involves careful consideration of deadlock detection and prevention, timeout settings, and escalation policies (converting many fine-grained locks to fewer coarse-grained locks). While locking ensures consistency, excessive locking can severely impact system performance through serialization and blocking. Database administrators must balance isolation requirements against performance needs when configuring locking behavior.",
    tips: [
      "Compare pessimistic and optimistic locking approaches",
      "Explain how different isolation levels affect locking behavior",
      "Discuss deadlock detection and prevention strategies",
      "Address lock escalation and its performance implications",
    ],
    tags: ["database", "concurrency", "transactions", "locking"],
    estimatedTime: 4,
    industry: ["tech"],
    practiceCount: 0,
    successRate: 0,
  },
  {
    id: "db-27",
    question:
      "What is the GROUP BY clause in SQL and how does it work with aggregate functions?",
    category: "technical",
    difficulty: "medium",
    type: "technical",
    sampleAnswer:
      "The GROUP BY clause in SQL divides rows into groups based on specified columns, allowing aggregate functions to be applied to each group independently. When using GROUP BY, each column in the SELECT list must either be included in the GROUP BY clause or be used within an aggregate function (like SUM, AVG, COUNT, MIN, MAX). The basic syntax is: SELECT column1, column2, AGGREGATE_FUNCTION(column3) FROM table GROUP BY column1, column2; The query execution process first filters rows using the WHERE clause (if present), then groups the remaining rows based on GROUP BY columns, and finally applies aggregate functions to each group. The HAVING clause can then filter groups based on aggregate results, similar to how WHERE filters rows. For example, to find the average salary by department: SELECT department_id, AVG(salary) as avg_salary FROM employees GROUP BY department_id; To filter groups, such as finding departments with average salary over 50000: SELECT department_id, AVG(salary) as avg_salary FROM employees GROUP BY department_id HAVING AVG(salary) > 50000; GROUP BY supports grouping by multiple columns, creating more granular groups. For instance, to analyze salary by department and job title: SELECT department_id, job_title, AVG(salary) as avg_salary FROM employees GROUP BY department_id, job_title; Some databases support advanced grouping operations: ROLLUP generates subtotals and grand totals; CUBE creates subtotals for all possible combinations of grouped columns; and GROUPING SETS allows specification of multiple grouping combinations in a single query. GROUP BY's efficiency depends heavily on database indexing—columns used in GROUP BY should typically be indexed to improve performance on large datasets.",
    tips: [
      "Explain the difference between WHERE and HAVING clauses",
      "Demonstrate GROUP BY with multiple columns and their hierarchical relationship",
      "Discuss common errors like selecting columns not in GROUP BY",
      "Explain advanced grouping options like ROLLUP and CUBE",
    ],
    tags: ["database", "sql", "aggregation", "group-by"],
    estimatedTime: 3,
    industry: ["tech", "data-analysis"],
    practiceCount: 0,
    successRate: 0,
  },
];
